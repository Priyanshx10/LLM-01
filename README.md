# 🧠 LLM Projects and Learning Hub

This repository contains a curated set of **Language Model (LLM) projects** and learning materials—built from scratch using **Python** and **PyTorch**. It is designed to help learners and researchers explore, experiment with, and understand the inner workings of LLMs through small, focused projects.

---

## 📚 What's Inside

### ✅ Project 01 – Character-Level Tiny LLM
- A minimal character-level text generator.
- **Key Concepts**: Character tokenization, sequence modeling, LSTM architecture, sampling.
- **Use Case**: Generates text one character at a time.

### ✅ Project 02 – Word-Level Tiny LLM
- A lightweight model that generates text word-by-word.
- **Key Concepts**: Word-level tokenization, vocabulary building, basic RNNs.
- **Use Case**: Predicts the next word given a sequence of previous words.

### 🚀 Project 03 – Transformer-Based Tiny LLM
- A compact implementation of a **Transformer** language model.
- **Key Concepts**: Multi-head self-attention, positional encoding, embeddings, autoregressive generation.
- **Use Case**: Trains on small text datasets and generates coherent sequences.

### 🧩 Upcoming Projects
- GRU-based LLM
- Scaled Transformers
- Training utilities (saving, loading, metrics)
- Web & CLI interfaces for generation
- Dataset integrations (TinyStories, Shakespeare, etc.)

---

## 🎓 Purpose

- Demystify LLMs by building them from scratch—**no heavy abstractions**.
- Learn by doing: tokenization, embeddings, RNNs/Transformers, and training.
- Promote **best practices** in dataset preparation, reproducibility, and experiment tracking.
- Encourage learners to **modify and extend** the projects.

---

## 🏗️ Getting Started

Each project is self-contained and includes:
- 🧠 Source code
- 📁 Sample `data.txt` file
- 📜 Project-specific README
- 🔧 Training scripts and utilities

### ✅ Quick Setup

```bash
# Clone the repo
git clone https://github.com/your-username/llm-projects.git
cd llm-projects/Project-01  # or any other project

🧠 Learning Objectives
By following these projects, you will:

Understand how LLMs process and generate text.
Learn tokenization techniques (character-level and word-level).
Train and evaluate sequence models (LSTM, Transformer).
Improve coding proficiency in PyTorch.
Explore generative capabilities and sampling techniques.

🤝 Contributing
We welcome contributions of all kinds:

New projects (GRU, GPT variants, BERT)
Code optimization or refactoring
Documentation and tutorial improvements
Datasets and use-case examples

📥 Submit a pull request or open an issue with your idea or fix.

📄 License
This project is licensed under the MIT License — feel free to use, modify, and share.
# Install dependencies
pip install -r requirements.txt
