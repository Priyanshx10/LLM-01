# ğŸš€ Project 03: Transformer-Based Mini LLM

Build your own mini **Language Learning Model (LLM)** from scratch using PyTorch â€” powered by the **Transformer** architecture. This project upgrades your understanding beyond LSTM and takes you into modern deep learning techniques.

---

## ğŸ§  What is this project?

This project is a word-level language model that uses the **Transformer** architecture â€” the same concept behind GPT and BERT. Unlike LSTMs that process words one by one, Transformers allow parallel processing and capture **long-range dependencies** more effectively.

---

## ğŸ“š Concepts Covered

| ğŸ§© Component             | ğŸ” Explanation                                                                   |
| ------------------------ | -------------------------------------------------------------------------------- |
| **Tokenization**         | Converts words into numerical indices (like `["hello", "world"]` â†’ `[3, 17]`).   |
| **Embedding**            | Turns each index into a dense vector (`[3, 17]` â†’ `[[0.2, 0.5], [0.9, 0.1]]`).   |
| **Positional Encoding**  | Adds position info so the model knows the word order.                            |
| **Self-Attention**       | Learns to focus on relevant words in a sequence, regardless of their position.   |
| **Multi-Head Attention** | Multiple attention mechanisms working in parallel to capture different patterns. |
| **Transformer Block**    | Layer made of attention + feed-forward network + layer norm + residuals.         |
| **Text Generation**      | Predicts the next word repeatedly to generate complete sentences.                |

---

## ğŸ—‚ï¸ Project Structure

transformer_llm/
â”‚
â”œâ”€â”€ data.txt # Raw training text
â”œâ”€â”€ transformer_llm.py # Main model & training script
â”œâ”€â”€ README.md # You're reading it!
â””â”€â”€ requirements.txt # Dependencies

## ğŸ› ï¸ Setup & Installation

```bash
# 1. Clone the repo or save the files
git clone https://github.com/your-username/transformer-llm
cd transformer-llm

# 2. Install dependencies
pip install -r requirements.txt

# 3. Add your training text (any .txt file)
# For example, data.txt = "hello world welcome to AI"

# 4. Run the model
python transformer_llm.py
```
